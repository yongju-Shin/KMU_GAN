{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GTyDZmDVBml"
   },
   "source": [
    "# HyperStyle Inference Notebook (on Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rvG7nRoGDUY"
   },
   "source": [
    "## Prepare Environment and Download HyperStyle Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRzve9Y5DucV",
    "outputId": "4cc48e0e-2f68-442d-81d1-9f6ebad7a23a"
   },
   "outputs": [],
   "source": [
    "#Clone HyperStyle Repo and Install Ninja \n",
    "import os\n",
    "os.chdir('/content')\n",
    "CODE_DIR = 'hyperstyle'\n",
    "\n",
    "## clone repo\n",
    "!git clone https://github.com/yuval-alaluf/hyperstyle.git $CODE_DIR\n",
    "\n",
    "## install ninja\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "\n",
    "os.chdir(f'./{CODE_DIR}')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxeMz-doE_pr"
   },
   "outputs": [],
   "source": [
    "#@title Import Packages { display-mode: \"form\" } \n",
    "import time\n",
    "import sys\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from notebooks.notebook_utils import Downloader, HYPERSTYLE_PATHS, W_ENCODERS_PATHS, run_alignment\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_inversion\n",
    "from utils.domain_adaptation_utils import run_domain_adaptation\n",
    "from utils.model_utils import load_model, load_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tozsg81kTKA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Inference Parameters\n",
    "\n",
    "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHz3Js0HEXbi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_ARGS = {\n",
    "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
    "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
    "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmOsCJWB6mGM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download Models\n",
    "\n",
    "We also need to verify that the model was downloaded correctly. All of our models should weigh approximately 1.3GB. Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ31J_m7kTJ8",
    "outputId": "464f6aca-e789-4f67-a6a0-9ae55193f9c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Download HyperStyle Model\n",
    "downloader = Downloader(code_dir=CODE_DIR, use_pydrive=True)\n",
    "print(f'Downloading HyperStyle model...')\n",
    "downloader.download_file(file_id=HYPERSTYLE_PATHS['faces']['id'], file_name=HYPERSTYLE_PATHS['faces']['name'])\n",
    "# if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "    raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "else:\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKeTTMU26mGN",
    "outputId": "7daed52e-5b3a-40c7-c26c-d549625d57f1"
   },
   "outputs": [],
   "source": [
    "# Download WEncoder Model \n",
    "print(f'Downloading the WEncoder model...')\n",
    "downloader.download_file(file_id=W_ENCODERS_PATHS['faces']['id'], file_name=W_ENCODERS_PATHS['faces']['name'])\n",
    "# if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "if os.path.getsize(EXPERIMENT_ARGS['w_encoder_path']) < 1000000:\n",
    "    raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "else:\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAWrUehTkTKJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the `EXPERIMENT_DATA_ARGS` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713,
     "referenced_widgets": [
      "7aa49c7f91d1423e8a5353cc635fd3a2",
      "4f0c66da5b7f4f39a2451eb4ed90a30d",
      "a1acd7c9e3cf491db9e7a997f36de061",
      "7c114635c9fb43a1b8364b485b75fda0",
      "b9b37def825f421fb7034b683b93d9c1",
      "54ca7bb3ad614ef6b39f420018573054",
      "629024e2721740189d13a78b1806e281",
      "a34dc8f9a29c41ee9fb16f60f75d09b0",
      "45cd754ee96e48ad83ebbb1cba1f7c04",
      "c7823a20876f46a9ba2ea5b251a25ae2",
      "afb6180d1d1c4cc3adbd61a5712d4e44"
     ]
    },
    "id": "1t-AOhP1kTKJ",
    "outputId": "239b5756-79ba-4a1d-a11f-a462de0ddc2f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load HyperStyle Model\n",
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
    "print('Model successfully loaded!')\n",
    "pprint.pprint(vars(opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdQ0fXM7ppSr"
   },
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJbNCfLaplKu"
   },
   "outputs": [],
   "source": [
    "def generate_mp4(out_name, images, kwargs):\n",
    "    writer = imageio.get_writer(out_name + '.mp4', **kwargs)\n",
    "    for image in images:\n",
    "        writer.append_data(image)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def get_latent_and_weight_deltas(inputs, net, opts):\n",
    "    opts.resize_outputs = False\n",
    "    opts.n_iters_per_batch = 5\n",
    "    with torch.no_grad():\n",
    "        _, latent, weights_deltas, _ = run_inversion(inputs.to(\"cuda\").float(), net, opts)\n",
    "    weights_deltas = [w[0] if w is not None else None for w in weights_deltas]\n",
    "    return latent, weights_deltas\n",
    "    \n",
    "\n",
    "def get_result_from_vecs(vectors_a, vectors_b, weights_deltas_a, weights_deltas_b, alpha):\n",
    "    results = []\n",
    "    for i in range(len(vectors_a)):\n",
    "        with torch.no_grad():\n",
    "            cur_vec = vectors_b[i] * alpha + vectors_a[i] * (1 - alpha)\n",
    "            cur_weight_deltas = interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha)\n",
    "            res = net.decoder([cur_vec],\n",
    "                              weights_deltas=cur_weight_deltas,\n",
    "                              randomize_noise=False,\n",
    "                              input_is_latent=True)[0]\n",
    "            results.append(res[0])\n",
    "    return results\n",
    "\n",
    "def interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha):\n",
    "    cur_weight_deltas = []\n",
    "    for weight_idx, w in enumerate(weights_deltas_a):\n",
    "        if w is not None:\n",
    "            delta = weights_deltas_b[weight_idx] * alpha + weights_deltas_a[weight_idx] * (1 - alpha)\n",
    "        else:\n",
    "            delta = None\n",
    "        cur_weight_deltas.append(delta)\n",
    "    return cur_weight_deltas\n",
    "    \n",
    "def show_mp4(filename, width):\n",
    "    mp4 = open(filename + '.mp4', 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display(HTML(\"\"\"\n",
    "    <video width=\"%d\" controls autoplay loop>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % (width, data_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCpzl1suFswz"
   },
   "source": [
    "## Define Input Images\n",
    "Define which images to include in the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6MdqDhDqDhb",
    "outputId": "2d49f773-a539-41f8-caf4-1d94279ae7ca"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "root_dir = \"./notebooks/images/animations\"\n",
    "image_paths = [e for e in glob(f'{root_dir}/*.jpg') if not e.endswith('_aligned.jpg')]\n",
    "image_names =[Path(e).stem for e in image_paths] \n",
    "print(image_names)\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvsWbbOytp4a"
   },
   "source": [
    "## Align Images\n",
    "Align the images if needed. You can skip this step if working on non-face images or if your images are pre-aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Bpzbr_Etoow",
    "outputId": "c1788467-8577-40f2-f854-a4b31d20c3ca"
   },
   "outputs": [],
   "source": [
    "# only align images if working on faces and if specified\n",
    "aligned_image_paths = []\n",
    "for image_name, image_path in zip(image_names, image_paths): \n",
    "    print(f'Aligning {image_name}...')\n",
    "    aligned_image = run_alignment(image_path)\n",
    "    aligned_path = os.path.join(root_dir, f'{image_name}_aligned.jpg')\n",
    "    # save the aligned image\n",
    "    aligned_image.save(aligned_path)\n",
    "    aligned_image_paths.append(aligned_path)\n",
    "# use the save aligned images as our input image paths\n",
    "image_paths = aligned_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA8pC10Yry6U"
   },
   "source": [
    "## Run Inference!\n",
    "Invert all images and interpolate between the latent codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFQyaD1Qqe3d",
    "outputId": "04910cf8-67ef-4e7a-b0ac-45a912ecd1d6"
   },
   "outputs": [],
   "source": [
    "#Run Inference! (This may take several minutes)\n",
    "in_images = []\n",
    "all_vecs = []\n",
    "all_weights_deltas = []\n",
    "\n",
    "resize_amount = (opts.output_size, opts.output_size)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    print(f'Working on {os.path.basename(image_path)}...')\n",
    "    original_image = Image.open(image_path)\n",
    "    original_image = original_image.convert(\"RGB\")\n",
    "    input_image = img_transforms(original_image)\n",
    "    # get the weight deltas for each image\n",
    "    result_vec, weights_deltas = get_latent_and_weight_deltas(input_image.unsqueeze(0), net, opts)\n",
    "    all_vecs.append([result_vec])\n",
    "    all_weights_deltas.append(weights_deltas)\n",
    "    in_images.append(original_image.resize(resize_amount))\n",
    "\n",
    "n_transition = 25\n",
    "SIZE = opts.output_size\n",
    "\n",
    "images = []\n",
    "image_paths.append(image_paths[0])\n",
    "all_vecs.append(all_vecs[0])\n",
    "all_weights_deltas.append(all_weights_deltas[0])\n",
    "in_images.append(in_images[0])\n",
    "\n",
    "for i in range(1, len(image_paths)):\n",
    "    if i == 0:\n",
    "        alpha_vals = [0] * 10 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
    "    else:\n",
    "        alpha_vals = [0] * 5 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
    "\n",
    "    for alpha in tqdm(alpha_vals):\n",
    "        image_a = np.array(in_images[i - 1])\n",
    "        image_b = np.array(in_images[i])\n",
    "        image_joint = np.zeros_like(image_a)\n",
    "        up_to_row = int((SIZE - 1) * alpha)\n",
    "        if up_to_row > 0:\n",
    "            image_joint[:(up_to_row + 1), :, :] = image_b[((SIZE - 1) - up_to_row):, :, :]\n",
    "        if up_to_row < (SIZE - 1):\n",
    "            image_joint[up_to_row:, :, :] = image_a[:(SIZE - up_to_row), :, :]\n",
    "\n",
    "        result_image = get_result_from_vecs(all_vecs[i - 1], all_vecs[i],\n",
    "                                            all_weights_deltas[i - 1], all_weights_deltas[i],\n",
    "                                            alpha)[0]\n",
    "\n",
    "        output_im = tensor2im(result_image)\n",
    "        res = np.concatenate([image_joint, np.array(output_im)], axis=1)\n",
    "        images.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV7yaSXar88q"
   },
   "source": [
    "## Save and Display Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "YqvG0oJtsUWt",
    "outputId": "88bde4aa-3ea5-4b5d-91c7-2ade7d6254b3"
   },
   "outputs": [],
   "source": [
    "kwargs = {'fps': 15}\n",
    "save_path = \"./notebooks/animations\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "gif_path = os.path.join(save_path, f\"faces_gif\")\n",
    "generate_mp4(gif_path, images, kwargs)\n",
    "show_mp4(gif_path, width=opts.output_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mdQ0fXM7ppSr"
   ],
   "name": "hyperstyle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "45cd754ee96e48ad83ebbb1cba1f7c04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f0c66da5b7f4f39a2451eb4ed90a30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54ca7bb3ad614ef6b39f420018573054",
      "placeholder": "​",
      "style": "IPY_MODEL_629024e2721740189d13a78b1806e281",
      "value": "100%"
     }
    },
    "54ca7bb3ad614ef6b39f420018573054": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "629024e2721740189d13a78b1806e281": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7aa49c7f91d1423e8a5353cc635fd3a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f0c66da5b7f4f39a2451eb4ed90a30d",
       "IPY_MODEL_a1acd7c9e3cf491db9e7a997f36de061",
       "IPY_MODEL_7c114635c9fb43a1b8364b485b75fda0"
      ],
      "layout": "IPY_MODEL_b9b37def825f421fb7034b683b93d9c1"
     }
    },
    "7c114635c9fb43a1b8364b485b75fda0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7823a20876f46a9ba2ea5b251a25ae2",
      "placeholder": "​",
      "style": "IPY_MODEL_afb6180d1d1c4cc3adbd61a5712d4e44",
      "value": " 83.3M/83.3M [00:05&lt;00:00, 14.8MB/s]"
     }
    },
    "a1acd7c9e3cf491db9e7a997f36de061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a34dc8f9a29c41ee9fb16f60f75d09b0",
      "max": 87319819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45cd754ee96e48ad83ebbb1cba1f7c04",
      "value": 87319819
     }
    },
    "a34dc8f9a29c41ee9fb16f60f75d09b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afb6180d1d1c4cc3adbd61a5712d4e44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9b37def825f421fb7034b683b93d9c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7823a20876f46a9ba2ea5b251a25ae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
