{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ID'] = 'PCI_BUS_ORDER'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "students = ['곽승준', '권용순', '김민환', '김봉상', '김용욱', '김형준', '노용문', '박영진', '신용주', '신현준', '안혜영', '오동규', '임항빈', '정근시', '조우성', '조윤정', '홍성규']\n",
    "CHOICE = random.choice(students)\n",
    "print(CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "MNIST    = 'mnist'        # https://en.wikipedia.org/wiki/MNIST_database\n",
    "F_MNIST  = 'fashion_mnist' # https://github.com/zalandoresearch/fashion-mnist\n",
    "CIFAR10 = 'cifar10'     # https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "DATA_NAME = CIFAR10 \n",
    "\n",
    "\n",
    "DATA_DIR = f'data/{DATA_NAME}'\n",
    "GEN_DIR = f'gen_images_{DATA_NAME}'\n",
    "os.makedirs(GEN_DIR, exist_ok=True)\n",
    "\n",
    "print(f'torch.version:{torch.__version__}')\n",
    "print(f'cuda:{cuda}')\n",
    "\n",
    "\n",
    "!python --version\n",
    "!pwd\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_channel = {MNIST:1, F_MNIST:1, CIFAR10:3}[DATA_NAME]\n",
    "img_size =  {MNIST:28, F_MNIST:28, CIFAR10:32}[DATA_NAME]\n",
    "VISION_DATASET =  {MNIST:datasets.MNIST, F_MNIST:datasets.FashionMNIST, CIFAR10:datasets.CIFAR10}[DATA_NAME]\n",
    "\n",
    "class Option:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 50         # number of epochs of training\n",
    "        self.batch_size = 64       # size of the batches\n",
    "        self.lr = 0.0002           # adam: learning rate\n",
    "        self.b1 = 0.5              # adam: decay of first order momentum of gradient\n",
    "        self.b2 = 0.999            # adam: decay of first order momentum of gradient\n",
    "        self.n_cpu = 2             # number of cpu threads to use during batch generation\n",
    "        self.latent_dim = 100      # dimensionality of the latent space\n",
    "        self.n_classes = 10        # number of classes for dataset\n",
    "        self.img_size = img_size   # size of each image dimension\n",
    "        self.channels = n_channel  # number of image channels\n",
    "        self.sample_interval = 400 # interval between image sampling\")\n",
    "        \n",
    "opt = Option()\n",
    "vars(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "    \n",
    "models['base'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 김형준\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = opt.channels\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = opt.latent_dim\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = opt.img_size\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = opt.img_size\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # https://simpling.tistory.com/entry/Embedding-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        # Linear를 ConvTranspose2d 로 변경\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.ReLU(True))\n",
    "            return layers\n",
    "        \n",
    "        # normalize 를 True 로 BatchNorm2d 를 탈 수 있도록 수정\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=True),\n",
    "            *block(128, 256, normalize=True),\n",
    "            *block(256, 512, normalize=True),\n",
    "            *block(512, 1024, normalize=True),\n",
    "            nn.ConvTranspose2d(1024, int(np.prod(img_shape)), 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        \n",
    "        # gen_input 값이 사이즈가 안맞아서 error [64, 110]\n",
    "        # Expected 4-dimensional input for 4-dimensional weight [110, 128, 4, 4], but got 2-dimensional input of size [64, 110] instead\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(opt.n_classes + int(np.prod(img_shape)), 512, 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 512, 4),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 512, 4),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "models['김형준'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 조우성\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_embedding = nn.Sequential(\n",
    "            nn.Embedding(opt.n_classes, 128),\n",
    "            nn.Linear(128, 16*4)\n",
    "        )\n",
    "        \n",
    "        self.noise_dense = nn.Sequential(\n",
    "            nn.Linear(opt.latent_dim, 4*4*256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ConvTranspose2d(in_c, out_c, k, s, p, bias)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(260, 64*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64*2, momentum=0.1, eps=0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64*2, 64*1, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64*1, momentum=0.1, eps=0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64*1, opt.channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        noise = self.noise_dense(noise)\n",
    "        noise = noise.view(-1, 256, 4, 4)\n",
    "        \n",
    "        labels = self.label_embedding(labels)\n",
    "        labels = labels.view(-1, 4, 4, 4)\n",
    "        \n",
    "        concat = torch.cat((noise, labels), dim=1)\n",
    "        \n",
    "        img = self.model(concat)\n",
    "\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_embedding = nn.Sequential(\n",
    "            nn.Embedding(opt.n_classes, 128),\n",
    "            nn.Linear(128, np.prod(img_shape))\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64*1, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64*1, 64*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64*2, momentum=0.1, eps=0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64*2, 64*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64*4, momentum=0.1, eps=0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        labels = self.label_embedding(labels)\n",
    "        labels = labels.view(-1, 3, 32, 32)\n",
    "        \n",
    "        concat = torch.cat((img, labels), dim=1)\n",
    "        \n",
    "        validity = self.model(concat)\n",
    "\n",
    "        return validity\n",
    "\n",
    "models['조우성'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.linear_model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 7*7*32, normalize=False)\n",
    "        )\n",
    "        self.deconv2d_model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, 2, 0, bias = False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(8, 3, 4, 2, 0, bias = False)\n",
    "        )\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.linear_model(gen_input)\n",
    "        img = img.view(img.size(0), *(32,7,7))\n",
    "        img = self.deconv2d_model(img)\n",
    "        img = self.tanh(img)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)        \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear((32 * 6 * 6) + 10, 512), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 1)        \n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        validity = self.features(img)\n",
    "        validity = self.avgpool(validity)\n",
    "        validity = torch.flatten(validity, 1)\n",
    "        validity = torch.cat((validity, self.label_embedding(labels)), -1)\n",
    "        validity = self.classifier(validity)\n",
    "        return validity\n",
    "\n",
    "models['오동규'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_ngf = 64              # size of generator filters\n",
    "opt_ndf = 64              # size of discriminator filters\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def conv_block(in_feat, out_feat, kernel_size, stride, padding, normalize=True, activation='leaky_relu'):\n",
    "            layers = [nn.ConvTranspose2d(in_channels=in_feat, out_channels=out_feat, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            activations = {'relu': nn.ReLU(True), 'leaky_relu': nn.LeakyReLU(0.01, inplace=True), 'tanh': nn.Tanh()}\n",
    "            layers.append(activations[activation])\n",
    "            return layers\n",
    "    \n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)   \n",
    "        self.upsampling = nn.Sequential(\n",
    "            *conv_block(opt.latent_dim + opt.n_classes, opt_ngf * 4, kernel_size=4, stride=1, padding=0, activation='relu'),\n",
    "            *conv_block(opt_ngf * 4, opt_ngf * 2, kernel_size=4, stride=2, padding=1, activation='relu'),\n",
    "            *conv_block(opt_ngf * 2, opt_ngf, kernel_size=4, stride=2, padding=1, activation='relu'),\n",
    "            *conv_block(opt_ngf, opt.channels, kernel_size=4, stride=2, padding=1, normalize=False, activation='tanh'), # 64 x 3 x 32 x 32\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        for _ in range(2):\n",
    "            gen_input = torch.unsqueeze(gen_input, -1)\n",
    "        img = self.upsampling(gen_input)\n",
    "        return img\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity \n",
    "    \n",
    "models['홍성규'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "        \n",
    "        #dcgan Generator 샘플코드에 in_feat, out_feat만 수정\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 라벨 임베딩 제거\n",
    "        # self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        \n",
    "        #dcgan Discriminator샘플에 파라메터 제한에 맞추고 Upsample 추가 in_feat, out_feat 수정\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(opt.channels, 512, kernel_size=3, padding=0, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=0, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            #nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=0, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        \n",
    "    # 라벨 파라메터 제거\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "models['김봉상'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module): #생성자 생성\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        #block 정의\n",
    "        def block(in_feat, out_feat, normalize=True): #선형함수 \n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                #배치 정규화실행 (차원 동일)\n",
    "                #activation function 은 leakyRelu 사용\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        #생성자 모델은 연속적인 여러개의 블럭을 가짐\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))), # 결과적으로 1024 \n",
    "            nn.Tanh() # tanh 사용하여 -1 ~ 1 사이에 값 배출\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # 노이즈함수, label값 뱉어냄\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape) #view 함수 사용하여 img 형태를 가질 수 있게함\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module): #판별자 생성\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512), #generator와는 다르게 linear함수 사용\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1), \n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "    \n",
    "models['김용욱'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 입력값은 Z이며 Transposed Convolution을 거칩니다.\n",
    "            nn.ConvTranspose2d(opt.latent_dim + opt.n_classes, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (512) x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (256) x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (128) x 16 x 16\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 64 x 32 x 32\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "            \n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "     \n",
    "            # (64) x 64 x 64)\n",
    "            nn.Conv2d(opt.n_classes + int(np.prod(img_shape)), 64, 4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128 x 32 x 32\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # (256 * 2) x 16 x 16\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # (512 * 4) x 8 x 8\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "    \n",
    "models['노용문'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        self.latent_embedding = nn.Sequential(\n",
    "            nn.Linear(opt.latent_dim, 100),#32*32 // 2),\n",
    "        )\n",
    "        self.condition_embedding = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes, 10),#32*32 // 2),\n",
    "        )\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            # sample model. It has nn.ConvTranspose2d(1, 3, 4, 1, 0, bias = False)\n",
    "            # First parameter = Channels of input (=1)\n",
    "            # Second parameter = Channels of output (=3)\n",
    "            # Third parameter = Kernel size (=4)\n",
    "            # Fourth parameter = stride (=1)\n",
    "            # fifth parameter = padding (=0)\n",
    "            \n",
    "            # (in - 1)*stride - 2*padding + (kernel-1) +1\n",
    "            \n",
    "            nn.ConvTranspose2d( 110, 32*32, 4, 1, 0),\n",
    "            nn.BatchNorm2d(32*32),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 1024, 4, 4])\n",
    "            nn.ConvTranspose2d( 32*32, 32*32 // 2, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32*32 // 2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 512, 7, 7])\n",
    "            \n",
    "            nn.ConvTranspose2d( 32*32 // 2, 32*32 // 4, 2, 2, 1),\n",
    "            nn.BatchNorm2d(32*32 // 4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 256, 12, 12])\n",
    "            \n",
    "            nn.ConvTranspose2d( 32*32 // 4, 32*32 // 8, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32*32 // 8),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 128, 12, 12])\n",
    "            \n",
    "            nn.ConvTranspose2d( 32*32 // 8, 32*32 // 16, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32*32 // 16),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 64, 23, 23])\n",
    "            \n",
    "            nn.ConvTranspose2d( 32*32 // 16, 32*32 // 32, 4, 1, 0),\n",
    "            nn.BatchNorm2d(32*32 // 32),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 32, 26, 26])\n",
    "            \n",
    "            nn.ConvTranspose2d( 32*32 // 32, 32*32 // 64, 4, 1, 0),\n",
    "            nn.BatchNorm2d(32*32 // 64),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            #torch.Size([64, 16, 29, 29])\n",
    "            \n",
    "            #nn.ConvTranspose2d( 32*32 // 64, 1, 4, 1, 0),\n",
    "            nn.ConvTranspose2d( 32*32 // 64, opt.channels, 4, 1, 0),\n",
    "            #torch.Size([64, 3, 32, 32])\n",
    "            \n",
    "            #nn.ConvTranspose2d( 32*32 // 32, opt.channels, 3, 1, 0),\n",
    "            \n",
    "            nn.Tanh()\n",
    "            \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 4, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d( 128, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 5, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d( 128, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 5, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 5, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 128, out_channels = 3, kernel_size = 5, stride=1, padding='same'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        #gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        #img = self.model(gen_input)\n",
    "        #img = img.view(img.size(0), *img_shape)\n",
    "        ################################################################\n",
    "        vec_latent = self.latent_embedding(noise)\n",
    "        vec_class = self.condition_embedding(self.label_emb(labels))\n",
    "        combined = torch.cat([vec_latent,vec_class], dim=1).reshape(-1, 110, 1, 1)\n",
    "        \n",
    "        #print(combined.shape)\n",
    "        img = self.model(combined)\n",
    "        ##################################################################\n",
    "        #gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        \n",
    "        #img = self.model(gen_input)\n",
    "        #img = img.reshape(img.shape[0],128,8,8)\n",
    "        #img = self.cnn_model(img)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        \n",
    "        self.cnn_model = nn.Sequential(\n",
    "            #nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, stride=1, padding=0), \n",
    "            nn.Conv2d(in_channels = opt.channels, out_channels = 6, kernel_size = 5, stride=1, padding=0), \n",
    "            #output size = (width-kernel+2*padding)/stride +1\n",
    "            #cifar10 : (32-5+2*0)/1+1 = 28\n",
    "            #MNIST : (28-5+2*0)/1+1 = 24\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #28/2 = 14\n",
    "            #12\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            #(14-5+2*0)/1+1 = 10\n",
    "            #(12-5+2*0)/1+1 = 8\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #pooling 5\n",
    "            #4     \n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            #######################################################################\n",
    "            \n",
    "            #nn.Linear(16 * 4 * 4+opt.n_classes, 512),\n",
    "            \n",
    "            nn.Linear(16 * 5 * 5+opt.n_classes, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        \n",
    "        x = self.cnn_model(img)\n",
    "        x = torch.cat((x.view(x.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.model(x)        \n",
    "\n",
    "        validity = x\n",
    "        return validity\n",
    "    \n",
    "    \n",
    "\n",
    "models['박영진'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # 입력데이터 Z가 가장 처음 통과하는 전치 합성곱 계층입니다.\n",
    "            nn.ConvTranspose2d( opt.latent_dim, 64 * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( 64, 3 , 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 위의 계층을 통과한 데이터의 크기. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        #self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 입력 데이터의 크기는 (nc) x 64 x 64 입니다\n",
    "            nn.Conv2d(opt.channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ndf) x 32 x 32\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 위의 계층을 통과한 데이터의 크기. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(64 * 8, 1 , 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity    \n",
    "\n",
    "models['신용주'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 5)]\n",
    "            #layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                # BatchNorm2d(N, C, H, W)\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.ConvTranspose2d(1024, int(np.prod(img_shape)), 5),\n",
    "            #nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        gen_input = gen_input.unsqueeze(2).unsqueeze(3)\n",
    "        #print(gen_input.shape) # torch.Size([64, 110])\n",
    "        #img = self.model(gen_input)\n",
    "        #img = img.view(img.size(0), *img_shape)\n",
    "        #return img\n",
    "        return self.model(gen_input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Conv2d(in_channels, out_channels, kernel_size, kernel_size, stride, padding) \n",
    "            # 필수 파라미터 : in_channels, out_channels, kernel_size(kernel mean is same filter)\n",
    "            #nn.Conv2d(opt.channels, opt.img_size, 5),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            #nn.Conv2d(opt.img_size, opt.n_classes + int(np.prod(img_shape)), 5),\n",
    "            #nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(opt.n_classes + int(np.prod(img_shape)), 512, 5),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 256, 5),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 128, 5),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 1, 5),\n",
    "            #nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 512),\n",
    "            #nn.Dropout(0.4),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 512),\n",
    "            #nn.Dropout(0.4),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        print(img.shape)\n",
    "        print(labels.shape)\n",
    "        d_in = d_in.unsqueeze(2).unsqueeze(3)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "models['조윤정'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        d = 128\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 512),\n",
    "            #nn.Dropout(0.4),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 512),\n",
    "            #nn.Dropout(0.4),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(512, 1),            \n",
    "            nn.Conv2d(1, d, 4, 2, 1),\n",
    "            nn.Conv2d(d, d*2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(d*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(d*2, d*4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(d*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(d*4, d*8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(d*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(d*8, 1, 4, 1, 0),            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity   \n",
    "    \n",
    "models['정근시'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        \n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "    \n",
    "models['안혜영'] = {'G':Generator(), 'D': Discriminator()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Conv2d(1024, int(np.prod(img_shape))),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "            \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*3*3,100),                                              \n",
    "            nn.ReLU(),\n",
    "            # [100,100] -> [100,10]\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "            \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # [100,100] -> [100,10]\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity        \n",
    "            \n",
    "try:\n",
    "    models['김민환'] = {'G':Generator(), 'D': Discriminator()}\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  num_params(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "    \n",
    "model = models[CHOICE]\n",
    "generator = model['G']\n",
    "discriminator = model['D']\n",
    "\n",
    "\n",
    "print(num_params(generator), num_params(models['base']['G']))\n",
    "print(num_params(discriminator), num_params(models['base']['D']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "ds = VISION_DATASET(DATA_DIR, train=True,download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), \n",
    "             transforms.ToTensor(), \n",
    "             transforms.Normalize([0.5], [0.5])\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display sample\n",
    "#for _ in range(10):\n",
    "#    img, label =  random.choice(ds)\n",
    "#    img = ((img + 1)/2*255).type(torch.uint8).permute((1,2,0)).squeeze()\n",
    "#    print(f'[{label}] {ds.classes[label]}')\n",
    "#    display(Image.fromarray(img.numpy()).resize((128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "def sample_image(n_width, epoch, n_label = 10):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = torch.Tensor(np.random.normal(0, 1, (n_width* n_label, opt.latent_dim))).type(FloatTensor)\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([label for label in range(n_label) for _ in range(n_width)])\n",
    "    labels = torch.Tensor(labels).type(LongTensor)\n",
    "    gen_imgs = generator(z, labels)\n",
    "    img_path = f\"{GEN_DIR}/{epoch:06d}.png\"\n",
    "    save_image(gen_imgs.data, img_path, nrow=n_width, normalize=True)\n",
    "    return img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    print(f'{epoch+1}/{opt.n_epochs}')\n",
    "    if epoch:\n",
    "        print(f'D loss: {d_loss.item():.04f},  G loss: {g_loss.item()}')\n",
    "        display(Image.open(img_path))\n",
    "        \n",
    "        \n",
    "    for i, (imgs, labels) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = FloatTensor(batch_size, 1).fill_(1.0)\n",
    "        fake  = FloatTensor(batch_size, 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(FloatTensor)\n",
    "        labels = labels.type(LongTensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))\n",
    "        gen_labels = LongTensor(np.random.randint(0, opt.n_classes, batch_size))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    img_path = sample_image(n_width=30, epoch=epoch, n_label=opt.n_classes)\n",
    "    ipd.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stylegan3]",
   "language": "python",
   "name": "conda-env-stylegan3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
